# PySpark DataFrame Operations in Google Colab

This repository contains a Google Colab notebook demonstrating various operations and transformations on PySpark DataFrames. The notebook is designed to serve as a practical guide for working with PySpark in a Google Colab environment, covering a wide range of data manipulation and analytical tasks.

## Description

The notebook showcases how to perform essential operations on PySpark DataFrames, including data loading, filtering, aggregation, and visualization. It provides hands-on examples for each concept, making it easy to understand and apply PySpark for big data processing.

## Libraries Used

- **PySpark**: The main library used for working with DataFrames and performing big data analysis.
- **Google Colab**: The environment where the notebook is executed, with PySpark set up for distributed computing.

## Functions & Methods Covered

- `groupBy()`
- `agg()`
- `filter()`
- `select()`
- `union()`
- `withColumn()`
- `drop()`
- `alias()`
- `show()`
- `count()`
- `distinct()`
- `orderBy()`
- `fillna()`
- `dropna()`

## Learnings

This notebook covers a variety of PySpark concepts, including but not limited to:

- **DataFrame Creation**: How to create a DataFrame from lists of tuples and CSV files.
- **Data Loading**: Loading and exploring data using PySpark in a Colab environment.
- **Filtering & Selecting Data**: Techniques for filtering rows and selecting specific columns from a DataFrame.
- **Aggregations**: Performing group-by operations, calculating sums, averages, and other aggregate functions.
- **Handling Missing Data**: Techniques for dealing with null values, including dropping and filling them.
- **Data Manipulation**: Adding, renaming, and dropping columns, along with row operations.
- **Data Sorting**: Ordering DataFrame rows by specific columns.
- **Data Analysis**: Computing statistics such as skewness, kurtosis, and performing complex filters.
- **Visualizations**: Plotting histograms, box plots, and violin plots using PySpark and external libraries.

## How to Use

Clone this repository and open the notebook in Google Colab to execute the code and experiment with PySpark DataFrame operations.
